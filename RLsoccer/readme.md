# ⚽ Google Research Football AI 캠프 매뉴얼

본 매뉴얼은 인공지능 축구 에이전트를 학습시키고 테스트하기 위한 **G-Football** 환경 설정 및 주요 파라미터 가이드입니다.

---

## 📋 1. 핵심 환경 설정 (Environment Setup)

AI 환경을 생성할 때 사용하는 `football_env.create_environment()`의 주요 인자들입니다.

| 파라미터 | 설명 | 추천 값 |
| --- | --- | --- |
| **`env_name`** | 실행할 축구 시나리오 이름 | `academy_empty_goal_close` |
| **`render`** | 화면 출력 여부 (`True`면 경기 화면이 뜸) | 학습 시 `False`, 테스트 시 `True` |
| **`representation`** | AI가 정보를 받는 방식 | 입문자/학습용: `simple115` |
| **`stacked`** | 이전 프레임 정보 중첩 여부 | 기본값 `False` |

---

## 🏟️ 2. 주요 시나리오 (`env_name`)

학생들의 수준이나 미션에 따라 시나리오를 변경할 수 있습니다.

* **`academy_empty_goal_close`**: 빈 문전으로 드리블해서 골 넣기 (입문용)
* **`academy_run_pass_and_score`**: 수비수를 피해 패스하고 골 넣기 (중급용)
* **`11_vs_11_easy_stochastic`**: 정식 11대11 경기 (난이도: 쉬움)
* **`11_vs_11_hard_stochastic`**: 정식 11대11 경기 (난이도: 어려움)

---

## 🧠 3. 관찰 데이터 방식 (`representation`)

AI가 경기 상황을 어떻게 데이터로 받아들일지 결정합니다.

1. **`raw`**: 선수 위치, 공 위치 등을 '딕셔너리(JSON)' 형태로 전달. (데이터 분석 학습에 용이)
2. **`simple115`**: 모든 상황을 115개의 숫자로 나열하여 전달. (강화학습 훈련 시 가장 속도가 빠르고 효율적)
3. **`pixels`**: 실제 게임 화면 픽셀 데이터를 전달. (딥러닝 이미지 처리 학습용)

---

## 🏃 4. AI의 행동 지시 (`Action Space`)

AI는 매 순간 **0번부터 18번까지의 숫자** 중 하나를 선택하여 행동합니다.

| 숫자 | 행동 | 설명 |
| --- | --- | --- |
| **0** | Idle | 가만히 있기 |
| **1 ~ 8** | Movement | 8방향 이동 (왼쪽, 위, 오른쪽, 아래 및 대각선) |
| **9** | Long Pass | 롱 패스 |
| **10** | High Pass | 높게 띄워주는 패스 |
| **11** | Short Pass | 짧은 패스 |
| **12** | Shot | 슈팅 |
| **13** | Sprint | 전력 질주 |
| **14** | Release Direction | 방향키 떼기 |
| **15** | Release Sprint | 질주 멈추기 |
| **16** | Sliding | 슬라이딩 태클 |
| **17** | Dribble | 드리블 시작/해제 |
| **18** | GoalKeeper | 골키퍼 조작 |

---

## 📊 5. PPO 학습 로그 지표 설명 (Metrics & Training Diagnostics)

PPO 에이전트는 환경과 상호작용(rollout)에서 경험을 수집하고, 정책(policy)과 가치함수(value)를 동시에 업데이트합니다. 아래 지표들은 학습 안정성, 보상 획득 빈도, 정책 변화 크기, 가치 예측 정확도, 학습 속도 등을 진단하는 데 사용됩니다.

| 지표 그룹 | 파라미터 | 설명 | 상태 해석 가이드 |
|---|---|---|---|
| **Rollout (환경 경험 수집)** | `ep_len_mean` | 에피소드 1회의 평균 길이(step 수) | 100~200이면 보통 적당, 너무 짧으면 목표 실패가 빠른 경우 |
| | `ep_rew_mean` | 에피소드 당 평균 누적 보상 | 1에 가까우면 목표/보상 행동이 자주 발생하는 좋은 신호 |
| **Time (실행 속도)** | `fps` | 초당 시뮬레이션 프레임 수 | 150+이면 환경 실행이 빠르고 효율적 |
| | `iterations` | PPO 업데이트 루프 반복 횟수 | 많을수록 정책 업데이트가 더 진행됨 |
| | `time_elapsed` | 학습에 소요된 총 시간(초) | 전체 학습 속도 참고용 |
| | `total_timesteps` | 환경에서 경험한 총 step 수 | 에이전트가 학습에 활용한 전체 경험량 |
| **Training (정책/가치 업데이트)** | `approx_kl` | 이전 정책 대비 현재 정책 변화 크기(KL 근사) | 0.005~0.02면 안정적, 너무 크면 정책이 튈 가능성 |
| | `clip_fraction` | PPO clipping에 걸린 업데이트 비율 | 0.05~0.15가 이상적, 0.147은 적당히 조절 중 |
| | `entropy_loss` | 정책 탐색 다양성(랜덤성) 손실 | 음수면 정상, 절댓값이 크면 아직 탐색 중 |
| | `policy_gradient_loss` | 정책이 보상을 높이는 방향으로 업데이트 중인지 | 음수면 정상(PPO에서 좋은 방향) |
| | `value_loss` | 가치 네트워크(reward 예측)의 오차 | 0에 가까우면 안정적 예측 |
| | `explained_variance` | 가치 네트워크가 reward 패턴을 설명 가능한 비율 | 0.7+이면 예측력이 좋아지고 있는 상태 |

---